"""
Example: RAG (Retrieval-Augmented Generation) with LangChain
Level 6: AI-Ready Government Data

‡∏£‡∏±‡∏ô: pip install langchain langchain-community faiss-cpu sentence-transformers
     python langchain_rag.py

‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ local embeddings ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ OpenAI API
"""

import json
from pathlib import Path


def load_documents():
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô documents"""
    data_dir = Path(__file__).parent.parent / "data"
    jsonl_path = data_dir / "energy_stats.jsonl"

    documents = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            record = json.loads(line)
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á document text
            doc_text = f"""
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ {record['region_th']} ({record['region_en']}) ‡∏õ‡∏µ {record['year']}

- ‡∏£‡∏´‡∏±‡∏™‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ: {record['region_code']}
- ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤: {record['consumption_gwh']:,} GWh
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤: {record['customers']:,} ‡∏£‡∏≤‡∏¢
- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï: {record['growth_rate']}%
- ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ï‡πà‡∏≠‡∏´‡∏±‡∏ß: {record.get('consumption_per_capita', 'N/A')} kWh/‡∏Ñ‡∏ô
- ‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: {record.get('avg_monthly_bill', 'N/A')} ‡∏ö‡∏≤‡∏ó

{record.get('text_description', '')}
""".strip()

            documents.append({
                "text": doc_text,
                "metadata": {
                    "region_code": record["region_code"],
                    "region_th": record["region_th"],
                    "year": record["year"]
                }
            })

    return documents


def create_vectorstore(documents):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á vector store ‡∏à‡∏≤‡∏Å documents"""
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain.schema import Document

        print("Creating vector store...")

        # Create Document objects
        docs = [
            Document(page_content=d["text"], metadata=d["metadata"])
            for d in documents
        ]

        # Use local embeddings (no API needed)
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

        # Create FAISS vector store
        vectorstore = FAISS.from_documents(docs, embeddings)

        print(f"‚úÖ Created vector store with {len(docs)} documents")

        return vectorstore

    except ImportError as e:
        print(f"‚ùå Missing dependency: {e}")
        print("Install with: pip install langchain langchain-community faiss-cpu sentence-transformers")
        return None


def simple_rag_demo(vectorstore):
    """Demo RAG ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LLM)"""
    print("\n" + "=" * 60)
    print("SIMPLE RAG DEMO (Retrieval Only)")
    print("=" * 60)

    questions = [
        "‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡πÑ‡∏´‡∏ô‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
        "‡∏†‡∏≤‡∏Ñ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà",
        "‡∏†‡∏≤‡∏Ñ‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏Å‡∏µ‡πà‡∏£‡∏≤‡∏¢",
    ]

    for question in questions:
        print(f"\n‚ùì Question: {question}")

        # Retrieve relevant documents
        docs = vectorstore.similarity_search(question, k=2)

        print("üìÑ Retrieved Documents:")
        for i, doc in enumerate(docs, 1):
            print(f"\n  [{i}] {doc.metadata['region_th']}")
            # Show first 200 chars
            preview = doc.page_content[:200] + "..."
            print(f"      {preview}")


def full_rag_demo(vectorstore):
    """Demo RAG ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏° (‡πÉ‡∏ä‡πâ LLM)"""
    print("\n" + "=" * 60)
    print("FULL RAG DEMO (with LLM)")
    print("=" * 60)

    try:
        from langchain.chains import RetrievalQA
        from langchain_community.llms import Ollama

        print("\nTrying to connect to Ollama...")

        # Use local LLM (Ollama)
        llm = Ollama(model="llama2", base_url="http://localhost:11434")

        # Create RAG chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
            return_source_documents=True
        )

        questions = [
            "‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏∞‡πÑ‡∏£",
            "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏Ñ‡∏Å‡∏•‡∏≤‡∏á‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏Ñ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô",
        ]

        for question in questions:
            print(f"\n‚ùì Question: {question}")

            result = qa_chain({"query": question})

            print(f"üí° Answer: {result['result']}")
            print(f"üìö Sources: {[d.metadata['region_th'] for d in result['source_documents']]}")

    except Exception as e:
        print(f"\n‚ö†Ô∏è  Full RAG demo requires Ollama running locally")
        print(f"   Error: {e}")
        print("\nTo run full RAG demo:")
        print("  1. Install Ollama: https://ollama.ai")
        print("  2. Run: ollama pull llama2")
        print("  3. Run this script again")


def create_qa_dataset(documents):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Q&A dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fine-tuning"""
    print("\n" + "=" * 60)
    print("CREATING Q&A DATASET FOR FINE-TUNING")
    print("=" * 60)

    qa_pairs = []

    # Generate Q&A pairs from data
    for doc in documents:
        region = doc["metadata"]["region_th"]
        text = doc["text"]

        # Extract info
        lines = text.split("\n")
        consumption = None
        growth = None
        customers = None

        for line in lines:
            if "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤" in line:
                consumption = line.split(":")[1].strip()
            elif "‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï" in line:
                growth = line.split(":")[1].strip()
            elif "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤" in line:
                customers = line.split(":")[1].strip()

        # Create Q&A pairs
        if consumption:
            qa_pairs.append({
                "instruction": f"{region}‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡πÉ‡∏ô‡∏õ‡∏µ 2566",
                "input": "",
                "output": f"{region}‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ {consumption} ‡πÉ‡∏ô‡∏õ‡∏µ 2566"
            })

        if growth:
            qa_pairs.append({
                "instruction": f"‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡πÉ‡∏ô{region}‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà",
                "input": "",
                "output": f"{region}‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤ {growth}"
            })

    # Save as JSONL for fine-tuning
    output_path = Path(__file__).parent.parent / "data" / "qa_finetune.jsonl"
    with open(output_path, "w", encoding="utf-8") as f:
        for qa in qa_pairs:
            f.write(json.dumps(qa, ensure_ascii=False) + "\n")

    print(f"‚úÖ Created {len(qa_pairs)} Q&A pairs")
    print(f"   Saved to: {output_path}")

    print("\nSample Q&A pairs:")
    for qa in qa_pairs[:3]:
        print(f"\n  Q: {qa['instruction']}")
        print(f"  A: {qa['output']}")


def main():
    print("=" * 60)
    print("LANGCHAIN RAG EXAMPLE - Level 6 AI-Ready Data")
    print("=" * 60)

    # Load documents
    documents = load_documents()
    print(f"\nLoaded {len(documents)} documents")

    # Create vector store
    vectorstore = create_vectorstore(documents)

    if vectorstore:
        # Simple RAG demo (retrieval only)
        simple_rag_demo(vectorstore)

        # Full RAG demo (with LLM)
        full_rag_demo(vectorstore)

    # Create Q&A dataset for fine-tuning
    create_qa_dataset(documents)

    print("\n" + "=" * 60)
    print("‚úÖ RAG demo complete!")
    print("=" * 60)


if __name__ == "__main__":
    main()
